dependecies: libxml2-dev pv make gcc

Compiling:
==========
Make sure that libxml2 headers are are accessible via /usr/include/libxml by installing libxml2-dev and running ```cd /usr/include && ln -s libxml/libxml2 .```
Run ```make```

Creating dataset:
=================
Download pages-articles.xml.bz2 from the database backup dumps from https://dumps.wikimedia.org
Name the dump wikidump.bz2
Run ```make nodes.bin```
Note: The wikitext parser (printlinks.c) that is usedis quite basic, so it might not detect all of the links

Running:
========
Run ```./route nodes.bin titles.txt``` to get a simple CLI interfaces
Usage:
A source
B destination
R

You can also specifiy multiple sources or destinations:
A source1
A source2
B dest1
B dest2
B dest3
R

Server:
=======
You can run a simple server by running node serv.js (this requires nodejs)
Access it via http://localhost:8080/

Algorith used (in route.c):
===========================
It's basically a bidirectional breadth first search
Details:
-: nodes marked with dist
>: buffers storing nodeRefs
}: nodes stored in buffer and marked

A->
   <-B


A---(>) forward buffer stops updating once match found
   |
 matches
   |
  <--B


A---
 {---B buffer is called New

nodes in originalA will now get marked using originalA buffer, because those nodes
may not have a backward ref pointing to them in order to conserve space. (see parselinks.c)

A---
-----B

Now we print the route using the dist_b of the nodes. node->dist_b will be set to 0 to detect duplicates

Known bugs:
===========
Redirects can cause reference duplications (where one page links twice to another page)
Wikitext parsing misses links that are generated by templates and are not present in the wikitext itself
Malformed input files can cause undefine behaviour in route and explore


Possible optimizations:
=======================
parselinks: Don't call realloc as often

Further reading:
================
There are also other projects that do the same thing, but are a bit more polished
